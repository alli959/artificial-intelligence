{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REI602M Machine Learning - Homework 3\n",
    "### Due: Sunday 7.1.2021\n",
    "\n",
    "**Objectives**: Classification, support vector machines, text classification\n",
    "\n",
    "**Name**: Alexander Guðmundsson, **email: ** alg35@hi.is, **collaborators:** (if any)\n",
    "\n",
    "Please provide your solutions by filling in the appropriate cells in this notebook, creating new cells as needed. Hand in your solution on Gradescope. Make sure that you are familiar with the course rules on collaboration (encouraged) and copying (very, very, bad)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) [Spam filtering, 30 points - This is based on a problem from Andrew Ng's CS229 machine learning course at Stanford]\n",
    "In recent years, spam on electronic newsgroups has been an increasing problem. Here, you will\n",
    "build a classifier to distinguish between \"real\" newsgroup messages, and spam messages.\n",
    "For this experiment, a set of spam emails and a set of genuine newsgroup messages have been obtained.\n",
    "Using only the subject line and body of each message, we’ll learn to distinguish\n",
    "between the spam and non-spam.\n",
    "All the files for the problem are in the file `email_spam.zip`.\n",
    "In order to get the text emails into a form usable by a off-the shelf classifier, some preprocessing on the\n",
    "messages has already been performed. You can look at two sample spam emails in the files `spam_sample_original`,\n",
    "and their preprocessed forms in the files `spam_sample_preprocessed*`. The first line in\n",
    "the preprocessed format is just the label and is not part of the message. The preprocessing\n",
    "ensures that only the message body and subject remain in the dataset; email addresses\n",
    "(EMAILADDR), web addresses (HTTPADDR), currency (DOLLAR) and numbers (NUMBER)\n",
    "were also replaced by the special tokens to allow them to be considered properly in the\n",
    "classification process. (In this problem, we’ll going to call the features \"tokens\" rather than\n",
    "\"words,\" since some of the features will correspond to special values like EMAILADDR.\n",
    "You don’t have to worry about the distinction.) The files `news_sample original` and\n",
    "`news_sample_preprocessed` also give an example of a non-spam mail.\n",
    "\n",
    "The work to extract feature vectors (i.e. classifier inputs) out of the documents has also been done for you, so you\n",
    "can just load in the design matrices (called document-word matrices in text classification)\n",
    "containing all the data. In a document-word matrix, the $i$-th row represents the $i$-th document/email,\n",
    "and the $j$-th column represents the $j$-th distinct token. Thus, the $(i,j)$-entry of\n",
    "this matrix represents the number of occurrences of the $j$-th token in the $i$-th document.\n",
    "\n",
    "For this problem, we’ve chosen as our set of tokens considered (that is, as our vocabulary)\n",
    "only the medium frequency tokens. The intuition is that tokens that occur too often or\n",
    "too rarely do not have much classification value. (Examples tokens that occur very often\n",
    "are words like \"the\", \"and\", and \"of\", which occur in so many emails and are sufficiently\n",
    "content-free that they aren’t worth modeling.) Also, words were stemmed using a standard\n",
    "stemming algorithm; basically, this means that “price,” “prices” and “priced” have all been\n",
    "replaced with “price,” so that they can be treated as the same word. For a list of the tokens\n",
    "used, see the variable file `tokenlist`.\n",
    "Since the document-word matrix is extremely sparse (has lots of zero entries), we have\n",
    "stored it in our own efficient format to save space. You don’t have to worry about this\n",
    "format. The file `read_spam_data.py` provides the function `read_matrix` to read in the document-word\n",
    "matrix and labels.\n",
    "\n",
    "Train a linear SVM on this dataset using the implementation in scikit-learn, `sklearn.svm.LinearSVC`\n",
    "with parameter $C=0.1$. Evaluate the accuracy on the test set for training sets of size 50, 100,\n",
    "200, 400, 800 and 1400 and for the full test set as well. What conclusions can you draw from the results?\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) To read the training and test data and the list of tokens behind the features use\n",
    "```python\n",
    "    trainMatrix, tokenlist, trainCategory = readMatrix('MATRIX.TRAIN')\n",
    "    testMatrix, tokenlist, testCategory = readMatrix('MATRIX.TEST')```\n",
    "\n",
    "2) To use the `sklearn.svm.LinearSVC` class, you start by calling the `fit` function which solves the SVM optimization problem for the given training set. You then either call `predict` to get predictions for the test set (or other data points) and subsequently evaluate the error rate/accuracy for the classifier \"manually\"; or you call the `score` function which performs the two operations (prediction and evaluation) in one go. This is completely analogous to how all the classifiers are used in scikit-learn. The Jupyter workbook vika02_logreg.ipynb shows how this is done with the logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alli959/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alli959/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alli959/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alli959/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alli959/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alli959/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alli959/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alli959/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alli959/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alli959/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alli959/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alli959/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alli959/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alli959/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alli959/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/alli959/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "At least one argument from \"cellColours\" or \"cellText\" must be provided to create a table.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-cc6b682caacd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mcolLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"trainSize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mrowLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"testSize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrowLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrowLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mtable\u001b[0;34m(cellText, cellColours, cellLoc, colWidths, rowLabels, rowColours, rowLoc, colLabels, colColours, colLoc, loc, bbox, edges, **kwargs)\u001b[0m\n\u001b[1;32m   3001\u001b[0m         \u001b[0mrowColours\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrowColours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowLoc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrowLoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolLabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolLabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3002\u001b[0m         \u001b[0mcolColours\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolColours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolLoc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolLoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3003\u001b[0;31m         edges=edges, **kwargs)\n\u001b[0m\u001b[1;32m   3004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/table.py\u001b[0m in \u001b[0;36mtable\u001b[0;34m(ax, cellText, cellColours, cellLoc, colWidths, rowLabels, rowColours, rowLoc, colLabels, colColours, colLoc, loc, bbox, edges, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcellColours\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcellText\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         raise ValueError('At least one argument from \"cellColours\" or '\n\u001b[0m\u001b[1;32m    731\u001b[0m                          '\"cellText\" must be provided to create a table.')\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: At least one argument from \"cellColours\" or \"cellText\" must be provided to create a table."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.svm import LinearSVC as SVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def data_subsample(X, y, n):\n",
    "    # Select a random subset of the training data\n",
    "    perm = np.random.permutation(len(y))\n",
    "    X_sub=X[perm[0:n],:]\n",
    "    y_sub=y[perm[0:n]]\n",
    "    return X_sub, y_sub\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./email_spam\")\n",
    "\n",
    "from read_spam_data import read_matrix\n",
    "\n",
    "\n",
    "\n",
    "#insert Data\n",
    "train_Matrix, tokenlist, trainCategory = read_matrix('./email_spam/MATRIX.TRAIN')\n",
    "test_Matrix, tokenlist, testCategory = read_matrix('./email_spam/MATRIX.TEST')\n",
    "\n",
    "#TestCount\n",
    "ntests = np.array([50,100,200,400,800,1400, len(train_Matrix)])\n",
    "fitter = SVC(C = 0.1)\n",
    "scores = [] # [trainsize, testsize, score]\n",
    "\n",
    "#create an array of scores by sample size\n",
    "for trainsize in ntests:\n",
    "    for testsize in ntests:\n",
    "        #finding subsamples\n",
    "        sub_train_Matrix, sub_trainCategory = data_subsample(train_Matrix, trainCategory, trainsize)\n",
    "        sub_test_Matrix, sub_testCategory = data_subsample(test_Matrix, testCategory, testsize)\n",
    "        \n",
    "        #adding the sub_training data to linearSVC\n",
    "        fitter.fit(sub_train_Matrix, sub_trainCategory)\n",
    "        #getting score from sub_test data\n",
    "        score = fitter.score(sub_test_Matrix, sub_testCategory)\n",
    "        temp = [trainsize, testsize, score]\n",
    "        scores.append(temp)\n",
    "scores = np.array(scores)\n",
    "\n",
    "#create a table\n",
    "colLabels = [\"trainSize\", \"accuracy\"]\n",
    "rowLabels = [\"testSize\", \"accuracy\"]\n",
    "plt.table(rowLabels = rowLabels, colLabels = colLabels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) [Tweet sentiment, 30 points] Many organizations are interested in analyzing whether given text segments such as news stories and tweets convey positive or negative feeling. In some cases, negative tweets can do significant to company reputation and they are forcde to respond. A system that can automatically analyze text for sentiment is therefore of value. Your task here is to classify tweets sentiment into one of the following categories positive, neutral or negative.\n",
    "\n",
    "In this exercise you see how raw text containing \"tweet extracts\" can be converted to feature vectors similar to those that were provided with problem 1). The pandas package is used to read the data from file (`np.genfromtxt` is cumbersome to use here) and scikit-learn used to convert text to features.\n",
    "\n",
    "a) [20 points] Create a random train/test split using `sklearn.model_selection.train_test_split` and then train a logistic regression classifier on the data using scikit-learn. Use the `multi_class='ovr'` switch to use the one-against-all strategy to handle K>2 classes and set the regularization parameter $C$ to 0.1 to avoid numerical problems during the optimization.\n",
    "\n",
    "Report the accuracy of your classifier, provide a confusion matrix and comment briefly on the results.\n",
    "\n",
    "*Comments:*\n",
    "\n",
    "1) The data used in this exercise comes from a Machine learning competition on Kaggle. For more details, see here: https://www.kaggle.com/c/tweet-sentiment-extraction You can examine the raw data in e.g. a text editor or Excel (always an important step!)\n",
    "\n",
    "2) The `CountVectorizer` class counts the occurence of each word in a segment of text. It does some filtering behind the scenes to remove extremely rare words as well as the most frequent ones. See the documentation for more details. You are free to experiment with the settings if you want.\n",
    "\n",
    "3) The `LabelEncoder` class is used to convert text labels to integers, e.g. \"positive\", \"neutral\" and \"negative\" to 1, 0, -1 which are then used as inputs to a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d992674c5f59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read sentiment data from text files and convert to vector-based features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Read sentiment data from text files and convert to vector-based features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# The text files are somewhat messy, clean-up is probably a good idea\n",
    "df = pd.read_csv('sentiment_train.csv')\n",
    "display(df.head())\n",
    "\n",
    "# Encode tweets using bag-of-words representation\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=0.95)\n",
    "X = vectorizer.fit_transform(df['text'].apply(lambda x: np.str_(x)))\n",
    "\n",
    "# Mapping used to identify original text from feature IDs\n",
    "inv_map = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "\n",
    "# Encode \"positive\", neutral\" and \"negative\" labels as integers\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['sentiment'])\n",
    "\n",
    "# It is important to check for class imbalance\n",
    "print(\"X: \", X.shape) # Sanity check\n",
    "for i in range(3):\n",
    "    print(\"Class {} ({}), count={}\".format(i, le.classes_[i], np.sum(y == i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) [Feature importance, 10 points] In feature selection (a.k.a. input variable selection) the goal is to identify which features are most relevant for a given classification task. By performing a careful selection of features, the performance of a classifier can often be improved significantly, in particular when data is limited. Alternatively, it can be interesting to identify a minimal set of features for acceptable performance (e.g. due to high costs of collecting/measuring the full set of features). Examining the features most relevant to the classification can also provide valuable insights into the data.\n",
    "\n",
    "A simple feature selection strategy uses the size of the weights in a linear classifier as measures of feature importance. The larger $\\theta_k$  is, the larger the role of the corresponding feature in the decision function. The strategy is therefore to rank the features according to $\\theta_k$.\n",
    "\n",
    "The weights are stored in the `coef_` attribute in the LogisticRegression class. This is a n_classes x n_features matrix. For each of the classes, obtain the names of the 10 highest ranking features (in terms of $\\theta$'s). Comment briefly on the results (you need to consider how the multi-class setting is treated in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) [Stochastic gradient descent for SVM, 40 points]. In this problem you are to implement a stochastic gradient descent algorithm for training a linear SVM. The model is $f_\\theta(x)=\\theta^T x$ (to include an intercept term you can simply set $x_0=1$ as before). The algorithm minimizes the SVM objective function\n",
    "$$\n",
    "   J(\\theta) = \\frac{\\lambda}{2}\\theta^T \\theta + \\frac{1}{n} \\sum_{i=1}^n \\max(0, 1-y^{(i)}\\theta^T x^{(i)}).\n",
    "$$\n",
    "The hinge loss $\\max(0, 1-z)$ is not differentiable at $z=1$ and this results in an objective function which is not differentiable everywhere, hence the gradient of $J(\\theta)$ is not defined everywhere. To deal with this, the SGD algorithm uses the *sub-gradient* of $J$ instead (see below). The algorithm starts from $\\theta^{(0)}=0$ and performs a fixed number of iterations. Step $k$ of the algorithm is as follows:\n",
    "\n",
    "Select $i$ uniformly at random from $[1,n]$\n",
    "\n",
    "$\\alpha^{(k)} = \\frac{1}{\\lambda k}$\n",
    "\n",
    "if $y^{(i)}~(\\theta^{(k)})^T x^{(i)} < 1 ~\\textrm{then}$\n",
    "\n",
    "$\\quad \\theta^{(k+1)} = \\theta^{(k)} - \\alpha^{(k)}(\\lambda \\theta^{(k)} - y^{(i)} x^{(i)})$\n",
    "\n",
    "else\n",
    "\n",
    "$\\quad \\theta^{(k+1)} = \\theta^{(k)} - \\alpha^{(k)} \\lambda \\theta^{(k)}$\n",
    "\n",
    "where $\\theta^{(k)}$ denotes the parameter *vector* in iteration $k$ and $\\lambda>0$ is a regularization hyper-parameter. The step size $\\alpha^{(k)}$ decays over the course of iterations (instead to being constant as we've seen previously). This helps to avoid overshooting the minimum.\n",
    "\n",
    "a) [30 points] Implement the SGD algorithm above. Train an SVM using your algorithm on the data in `synth_train.txt` with $\\lambda=1/100$. Create a scatter plot of the training data (it is a 2D toy data set) and show the decision boundary of the classifier (see Jupyter workbook vika02_logreg). Report the model coefficients and the test set error (fraction of incorrectly classified examples) using the data in `synth_test.txt`\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) To sample uniformly at random from $[0,n-1]$ use `np.random.randint`.\n",
    "\n",
    "2) Use `np.genfromtxt` to read the data.\n",
    "\n",
    "3). A *sub-gradient* is a generalization of the gradient for convex functions which are not necessarily differentiable. Such functions arise quite frequently in machine learning, e.g. when the 1-norm is used for regularization. The sub-gradient of a function at a point is the slope of *a* hyperplane that passes through the point and lies below the graph of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_sgd(X, y, lambda_par, max_epochs=10):\n",
    "    # Inputs:\n",
    "    #   X ... Input variables (n x p matrix)\n",
    "    #   y ... Labels (n vector), -1 or 1\n",
    "    #   lambda_par ... Regularization constant (non-negative)\n",
    "    #   max_epochs ... Maximum number of passes through the data set\n",
    "    #\n",
    "    # Output:\n",
    "    #   Model coefficients (n vector)\n",
    "\n",
    "    assert(lambda_par > 0)\n",
    "    max_iter = max_epochs*X.shape[0]\n",
    "\n",
    "    # Insert code here\n",
    "    # ...\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) [10 points] Modify the code in a) so that it keeps track of the objective function value during the course of the iterations. Plot the objective function values as a function of iteration number. This is similar to what you did in homework 2 (but with a different objective function).\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) Computing the function values and training set error requires a pass through all the training data. This is computationally expensive, so you should compute these values once every $T$ iterations where $T$ could e.g. be 100, 1000 or $n$.\n",
    "\n",
    "2) To speed up the computations, use matrix and vector operations instead of *for*-loops where possible. For example, if the training set is in matrix $X$ you can classify all the examples in a single matrix-vector multiplication, $y_{pred}=X\\theta$ (why?) This issue is discussed in some detail in http://cs229.stanford.edu/section/vec_demo/Vectorization_Section.pdf\n",
    "  \n",
    "3) Note that the $\\lambda$ parameter in the above SVM formulation is related to the $C$ parameter in the \"standard\" SVM formulation via $\\lambda=1/(nC)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
