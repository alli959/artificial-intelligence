{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REI602M Machine Learning - Homework 5\n",
    "### Due: *Sunday* 21.2.2021\n",
    "\n",
    "**Objectives**: The AdaBoost classifier. Visualization of high-dimensional data with PCA and t-SNE, Non-negative matrix factorizaton for topic extraction.\n",
    "\n",
    "**Name**: (your name here), **email: ** (your email here), **collaborators:** (if any)\n",
    "\n",
    "Please provide your solutions by filling in the appropriate cells in this notebook, creating new cells as needed. Hand in your solution on Gradescope, taking care to locate the appropriate page numbers in the PDF document. Make sure that you are familiar with the course rules on collaboration (encouraged) and copying (very, very, bad).\n",
    "\n",
    "Note: The Adaboost exercise involves more programming than usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\\. [Visualization of high-dimensional data, 20 points]\n",
    "In this problem you will use PCA and t-SNE to visualize a high-dimensional data set derived from 300 Wikipedia articles selected from few broad groups of topics. For each Wikipedia article, the most common words such as 'an' and 'the' were removed and the rest of the words run through a stemming algorithm (converting e.g. 'longer' and 'longest' to 'long'). This resulted in a dictionary of all the words that occur in the 300 articles. The total number of words was 1000. A 1000-element histogram vector was then constructed for each article, where element $j$ is the frequency of word $j$ in the document, i.e. a 300 by 1000 matrix. The data is stored in the Numpy file `wikipedia_corpus.npz`.\n",
    "\n",
    "a) [10 points] Use PCA to create a 2D figure where each article in the figure is represented by a short string based on its title.\n",
    "\n",
    "b) [10 points] Use the t-SNE code provided with the assignment to create a similar figure to the one in a). You need to try a few different values of the perplexity parameter before you get a nice projection (include only the best one in your report). Can you \"squeeze\" more titles into this figure than the one in a)? \n",
    "\n",
    "What, if anything, can you conclude from your visualization experiments?\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) The `wikipedia_corpus.npz` file contains three arrays which you access as follows\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "data=np.load('wikipedia_corpus.npz', allow_pickle=True)\n",
    "dictionary = data[\"dictionary\"]\n",
    "article_titles = data[\"article_titles\"]\n",
    "article_histograms = data[\"article_histograms\"] # Data matrix```\n",
    "\n",
    "2) Creating informative figures usually takes some effort, so expect to spend some time tinkering with your figure. See http://www.cs.toronto.edu/~hinton/turian.png for an example of how your figure could look like.\n",
    "\n",
    "3) You should try to use as large figure as possible, use `plt.figure(figsize=(xsize,ysize))`\n",
    "\n",
    "4) You can only display titles of 100 -- 150 articles in the figure, otherwise you are likely to end up with a black mess.\n",
    "\n",
    "5) Some of the titles are quite long and you should therefore truncate them somehow, e.g. by keeping only the two first words in the title. Useful Python's string `split` and `join` methods may come in handy. Use `plt.text` to display text in the figure.\n",
    "\n",
    "6) Use PCA from scikit and the t-SNE code provided with this assignment (taken from https://lvdmaaten.github.io/tsne/). Note that for high-dimensional data, PCA is typically employed as a pre-processing step prior to running t-SNE in order to reduce the dimensionality of the data (typically to 30 - 50 dimensions). This reduces noise and speeds up the stochastic gradient descent in t-SNE. The preprocessing step is done automatically in the code supplied with the assignment but when you use the implementation in scikit-learn you must perform the PCA step manually.\n",
    "\n",
    "7) While t-SNE is very useful, there are several caveats one should be aware of. See e.g.\n",
    "https://distill.pub/2016/misread-tsne/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a)\n",
    "# Insert your code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b)\n",
    "# Insert your code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\\. [Topic discovery with NMF, 40 points]\n",
    "Here you will use non-negative matrix factorization (NMF)to analyze the content of a collection of articles from the New York Times. In particular, you will attempt to discover the main topics of the article collection by performing non-negative matrix factorization to a term-document matrix derived from the raw articles.\n",
    "\n",
    "The NMF approximates a *non-negative* $n \\times p$ matrix $X$ of rank $r$ with a rank $k \\leq r$ matrix such that\n",
    "\n",
    "$$\n",
    "X \\approx WH\n",
    "$$\n",
    "\n",
    "where $W$ is a $n \\times k$ matrix with $W_{ij} \\geq 0$ and $H$ is a $k \\times p$ matrix with $H_{ij} \\geq 0$. Provided that $k$ is appropriately chosen, the *weight matrix* $W$ and *coefficient matrix* $H$ can reveal interesting structures in the data. Column $j$ of $X$ is approximated with (see comment 1 below)\n",
    "\n",
    "$$\n",
    "X_{:,j} \\approx (WH)_{:,j} = H_{1j}W_{:,1} + H_{2j}W_{:,2} + \\ldots + H_{kj}W_{:,k}\n",
    "$$\n",
    "\n",
    "where the subscript $:,j$ denotes column $j$. When the columns of $X$ correspond to individual articles, the columns of $W$ correspond to the main topics in the articles and column $j$ of $H$ contains information on how the topics are \"mixed\" together to form (approximately) column $j$ of $X$.\n",
    "\n",
    "a) [30 points] Download the NYT articles from https://notendur.hi.is/steinng/kennsla/2021/ml/data/NYT.zip. Create a document-term matrix using word counts or TF-IDF (see below). For a given value of $k$, perform NMF on the *transposed* matrix and list the words corresponding to the largest $W_{ij}$ values for columns $j=1,\\ldots,k$. You need to experiment with different values of $k$ to get interesting topic groupings. If $k$ is too low different topics will be mixed together, when $k$ gets large, the same subject will appear in multiple groups. Report your results (c.a. 20 words on each topic) for the value of $k$ that you end up picking.\n",
    "\n",
    "b) [10 points] Select two or three topics that you find interesting. Identify the corresponding columns in $W$ and examine 3 - 5 articles using the largest $H$-values as indices. How does the content of the articles match the selected topics?\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) The $n \\times k$ matrix-vector product $y=Ax$ can be interpreted as a weighted sum of the columns of $A$,\n",
    "$$\n",
    "y=\n",
    "\\begin{array}{ccc}\n",
    "~\\mid &  & ~\\mid \\\\\n",
    "x_1 a_1 & + \\ldots + & x_k a_k \\\\\n",
    "~\\mid & & ~\\mid \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "and matrix multiplication can be considered as multiple matrix-vector products.\n",
    "\n",
    "2) Use the NMF implementation in`from sklearn.decomposition.NMF`. You can use the Wikipedia data set from problem 1) to test your NMF-based topic discovery code. Once you get convincing results, apply your code to the NYT data.\n",
    "\n",
    "3) Use `sklearn.feature_extraction.text.CountVectorizer` to create the document-term matrix based on word counts from the raw newspaper articles. This function performs tokenization, counting, normalization and removes stop words. You can try the following parameter values `max_df=0.95` (remove words that occur in at least 95% of the documents), `min_df=10` (remove words that occur in fewer than ten documents), `stop_words='english'`. You should specify a value for `max_features=` (default is all).\n",
    "\n",
    "4) Use `CountVectorizer.get_feature_names()` to get the list of words that were retained. Sidenote: Rare words are downplayed by the term-frequency encoding used here but they are often found to be informative. Therefore people often encode documents using term-frequency-inverse document frequency (`TfidfVectorizer` in Scikit-learn).\n",
    "\n",
    "5) Scikit-learn's NMF function obtaines the factorization $X \\approx WH$ by minimizing the objective function $0.5||X - WH||_F^2$ (where $||A||_F$ denotes the Frobenius norm of a $n \\times p$ matrix $A$, $||A||_F = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^p A_{ij}^2}$. The NMF implementation provides means to regularize the solution via parameters `alpha` and `l1_ratio`. You may want to experiment with these parameters to see if you can improve the list of topics.\n",
    "\n",
    "6) The $H$ matrix is stored in `nmf.components_`\n",
    "\n",
    "7) The NMF is described briefly in section 14.6 of ESL. A more detailed account can be found in the original article\n",
    "http://www.cs.columbia.edu/~blei/fogm/2020F/readings/LeeSeung1999.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the NYT articles from a text file. Code adapted from a workbook found on Kaggle.\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "NYTFile = open('nytimes_news_articles.txt', encoding='utf-8')\n",
    "NYTFile.seek(0)\n",
    "raw = NYTFile.readlines()\n",
    "articles = []\n",
    "sent = ''\n",
    "\n",
    "for i in range(0, len(raw) - 1):\n",
    "    if re.findall('URL',raw[i]) == []:\n",
    "        sent = sent + raw[i]\n",
    "        if (re.findall('URL', raw[i+1]) != []) and (i+1 < len(raw)):\n",
    "            articles.append(sent.strip())\n",
    "    else:\n",
    "        sent = ''\n",
    "print(\"Number of articles:\", len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a)\n",
    "# Insert your code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b)\n",
    "# Insert your code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\\. [AdaBoost classifier, 40 points]\n",
    "\n",
    "Implement the AdaBoost algorithm described on page 339 in ESL using decision tree stumps (root node + two leaf nodes) as base classifiers and redo the computations for the example of Figure 10.2 in ESL. Plot the training and test errors as a function of iteration number. Discuss briefly your findings. \n",
    "\n",
    "Repeat the experiments using a decision tree that captures 2-way feature interactions (3 leaf nodes). How do the results differ from the previous one?\n",
    "\n",
    "Use the following artifical data set. The features $x_1^{(i)},\\ldots,x_{10}^{(i)}$ are standard independent normally distributed variables and the output is defined as $y^{(i)}=1$ if $\\sum_{j=1}^{10} (x^{(i)}_j)^2 > 9.34$ and zero otherwise (see comments below).\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) Use `sklearn.tree.DecisionTreeClassifier` to generate the decision trees. Tree stumps are obtained by setting `max_leaf_nodes=2`. The behaviour of your ensemble classifier may be somewhat different from the one shown in Figure 10.2 in ESL since the implementation of the base tree classifier is different. For this reason you can use e.g. 1000 boosting iterations instead of 400.\n",
    "\n",
    "2) The `DecisionTreeClassifier.fit` function accepts a vector of sample weights as an optional argument.\n",
    "\n",
    "3) The training and test data is generated by the function `hw5_data` below. Use $n=1000$ samples for training and testing.\n",
    "\n",
    "4) If $\\text{err}_m$ becomes zero we are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hw5_data(n=1000):\n",
    "    # Toy dataset from page 339 in ESL\n",
    "    X=np.random.randn(2*n,10)\n",
    "    y=2*(np.sum(X**2,axis=1)>9.34)-1\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
