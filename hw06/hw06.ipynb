{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REI602M Machine Learning - Homework 6\n",
    "### Due: *Sunday* 28.2.2021\n",
    "\n",
    "**Objectives**: k-means clustering and recommender systems\n",
    "\n",
    "**Name**: Alexander Guðmundsson, **email: ** (alg35@hi.is), **collaborators:** (if any)\n",
    "\n",
    "Please provide your solutions by filling in the appropriate cells in this notebook, creating new cells as needed. Hand in your solution on Gradescope, taking care to locate the appropriate page numbers in the PDF document. Make sure that you are familiar with the course rules on collaboration (encouraged) and copying (very, very, bad)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\\. [Topic discovery via $k$-means, 30 points]\n",
    "\n",
    "Here you are to use the $k$-means algorithm to cluster the Wikipedia data set from Homework 5 (file `wikipedia_corpus.npz`).\n",
    "\n",
    "Run $k$-means with different values of $k$, e.g. $k=2,5,8$ and investigate your results by looking at the words and article titles associated with each centroid. Feel free to visit Wikipedia if an article’s content is unclear from its title. On the basis of your tests, select a final value of $k$ and run $k$-means again. Give a short description of the topics your clustering discovered along with the 5 most common words from each topic. If the topics do not make sense pick another value of $k$.\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) When you run the $k$-means implementation in `sklearn.cluster.KMeans` it initializes the centroids by randomly assigning the data points to $k$ groups and taking the $k$ representatives as the means of the groups. (This means that if you run the function twice, with the same data, you might get diﬀerent results.) The cluster centers and labels can be accessed via the attributes `cluster_centers_` and `labels_`. The attribute `labels_` contains the index of each vector’s closest centroid (labels start from zero), so if the 30th entry in `labels` is 7, then the 30th vector’s closest centroid is the 7th entry in `centroids` (indexing starts from zero).\n",
    "\n",
    "2) There are many ways to explore your results. For example, you could print the titles of selected articles in a cluster. Alternatively, you could ﬁnd a topic’s most common words by ordering `dictionary` by the size of its centroid’s entries. A larger entry for a word implies it was more common in articles from that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KMeans' object has no attribute 'cluster_centers_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a8a3ebb36204>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0my_kmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_histograms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KMeans' object has no attribute 'cluster_centers_'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "data = np.load('./../homework05/wikipedia_corpus.npz', allow_pickle=True)\n",
    "dictionary = data[\"dictionary\"]\n",
    "article_titles = data[\"article_titles\"]\n",
    "article_histograms = data[\"article_histograms\"] #Data matrix\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters = 5, random_state = 42)\n",
    "print(kmeans.cluster_centers_)\n",
    "print(kmeans.labels_)\n",
    "y_kmeans = kmeans.fit_predict(article_histograms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\\. [Recommender systems, 70 points]\n",
    "\n",
    "Here you will build a recommendation system for movie ratings, using data from the MovieLens web site. We start with a ''small'' data set with approx. 90,000 ratings of 3650 movies from 610 users and then move on to a larger set with\n",
    "one million ratings from 6000 users on 4000 movies.\n",
    "\n",
    "Each user typically only rates a few movies but we want to be able to predict ratings for all the movies the user hasn't rated. We do this by basing predictions for the unseen movies on ratings from all the other users using a method based on matrix factorization. We construct a rank-$k$  $n \\times m$ user-movies matrix $R$ of movie ratings where $r_{ui}$ corresponds to the rating that user $u$ would give movie $i$. The prediction $\\hat{r}_{ui}$ is given by\n",
    "$$\n",
    "\\hat{r}_{ui} = w_u^T h_i\n",
    "$$\n",
    "where $w_1^T,\\ldots,w^T_n$ are row vectors with $k$ elements and $h_1,\\ldots,h_m$ are column vectors with $k$ elements (see the article referenced below for an interpretation of these vectors).\n",
    "\n",
    "The task is to \"learn\" the elements of the $w$ and $h$ vectors from the available ratings. This is done by minimizing the least squares error,\n",
    "$$\n",
    "\\sum_{(u,i) \\in Z}(r_{ui} - w_u^T h_i)^2 + \\lambda (\\sum_{u=1}^n ||w_u||^2 + \\sum_{i=1}^m ||h_i||^2)\n",
    "$$\n",
    "where $Z$ is the set of available ratings (the training set) and $\\lambda >0$ is a regularization parameter that is used to avoid overfitting.\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) The MovieLens datasets are taken from https://grouplens.org/datasets/movielens/\n",
    "\n",
    "2) Start with the `ml-latest-small` data set. Once your code is working, you may want to switch to the `ml-1m` data set to obtain a more accurate model.\n",
    "\n",
    "3) Code for reading the MovieLens data and performing some cleanup, is given below.\n",
    "\n",
    "4) The rank $k$ is user defined. For the NetFlix dataset, a value of $k=40$ worked well (feel free to experiment).\n",
    "\n",
    "5) The recommendation systems studied here are based on an article by the winners of the NetFlix prize in 2009.\n",
    "https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf\n",
    "\n",
    "6) Minimizing the mean square error does not necessarily translate to better business\n",
    "\n",
    "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.465.96&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) [Baseline model, 20 points]\n",
    "Start by exploring the data briefly, e.g. by looking at the number of ratings behind each movie and the number of ratings per user (histograms are useful here).\n",
    "\n",
    "It is by no means guaranteed that a fancy machine learning model performs better than a simple model in the real world. Here we construct a simple baseline model which we use to gauge the quality of the matrix factorization model below. The baseline model is\n",
    "$$\n",
    "r_{ui} = \\mu + c_u + d_i\n",
    "$$\n",
    "where $\\mu \\in \\mathbb{R}$ is the average rating over all movies, $c\\in \\mathbb{R}^n$ is a vector representing the deviation of individual users from the average. If e.g. $c_u=-0.5$ then user $u$ tends to rate films 0.5 lower than the average. Element $i$ of the vector $d\\in \\mathbb{R}^m$ represents the deviation of film $i$ from the average. A positive $d_i$ indicates that movie $i$ is better than an average movie.\n",
    "\n",
    "Estimate $\\mu,~c$ and $d$ from the ratings data using least squares, i.e. by minimizing\n",
    "$$\n",
    "\\sum_{(u,i) \\in Z}(r_{ui} - \\mu - c_u - d_i)^2.\n",
    "$$\n",
    "This can be done by solving a standard least squares problem on the form $Ax \\approx b$. The vector $b$ contains the movie ratings, the vector $x=(c,d,\\mu)$ is an $n+m+1$ vector of unknowns. If rating $j$ is $(u,i,r)$ then $b_j=r$ and row $j$ of $A$ is as follows. All the elements of row $j$ are zero except $A[j,u-1]=1$, $A[j,n+i-1]=1$ and $A[j,n+m]=1$ (adjustments for zero-based indexing in numpy). The least squares problem is most conveniently solved using 'scipy.sparse.lsqr' after constructing the matrix with 'scipy.sparse.lil_matrix' and/or 'scipy.sparse.csc_matrix'. It is also possible to use stochastic gradient descent to obtain the parameter values.\n",
    "\n",
    "When you have obtained estimates of the model parameters, use the model to compute the *root mean square error* (RMSE) on the test set. Report the error, $\\mu$ and the first 10 elements of both the $c$ and $d$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) [Matrix factorization model, 20 points] Use stochastic gradient descent to minimize\n",
    "$$\n",
    "\\sum_{(u,i) \\in Z}(r_{ui} - w_u^T h_i)^2 + \\lambda (\\sum_{u=1}^n ||w_u||^2 + \\sum_{i=1}^m ||h_i||^2)\n",
    "$$\n",
    "with e.g. $k=20$. You can start with e.g. $\\lambda=0.01$ and step-size $\\alpha=0.05$ (some adjustments may be needed). To initialize the $w_u$ and $h_i$ vectors use normally distributed random values, e.g. with mean zero and standard deviation 0.02. You can perform the updates on the $w_u$ and $h_i$ vectors separately.\n",
    "\n",
    "Monitor the root mean square error on both the training set and the test set. Keep track of the best set of parameter values ($w$ and $h$ vectors) and stop training when the test error starts to increase.\n",
    "\n",
    "Report the root mean square error on the test set for the best parameters. How does this model compare to the one you found in a) in terms of RMSE?\n",
    "\n",
    "*Comment*: You can generate normally distributed random variables with `np.random.randn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) [Matrix factorization model with bias, 10 points] Expand your model from b) by adding user-movie bias on the form\n",
    "$$\n",
    "b_{ui} = \\mu + c_u + d_i.\n",
    "$$\n",
    "where $\\mu$ is the global average of the ratings,\n",
    "$$\n",
    "\\mu=\\sum_{(u,i) \\in Z} r_{ui} / |Z|\n",
    "$$ (fixed throughout the iterations) but the vectors $c$ and $d$ are estimated along with the $w$ and $h$ vectors. The predictive model becomes\n",
    "$$\n",
    "\\hat{r}_{ui} = \\mu + c_u + d_i + w_u^T h_i\n",
    "$$\n",
    "and the least squares error criteria\n",
    "$$\n",
    "\\sum_{(u,i) \\in Z}(r_{ui} - \\mu - c_u - d_i - w_u^T h_i)^2 + \\lambda (\\sum_{u=1}^n ||w_u||^2 + \\sum_{i=1}^m ||h_i||^2 + ||c||^2 + ||d||^2)\n",
    "$$\n",
    "Test different values of $k$ and $\\lambda$. Report the RMSE on the test set for the best model that you obtain. How does it compare to the models from a) and b)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) [Model evaluation in the real world - open ended, 10 points] Use the best model from a) - c) to generate movie recommendations for a user that reflects your own taste in movies, or a user that has strong preference for particular genre(s) (e.g. a horror fan). Many recommender systems suffer ''popularity bias'', i.e. they tend to focus on popular items. Does your model have this tendency? Discuss briefly. Do you think that your model is useful in the real world?\n",
    "\n",
    "Popularity bias in recomender systems and evaluation metrics are discussed in some detail here: https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.465.96&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is for reading the MovieLens data. You can place it wherever you want in this workbook or hide it away in a function that you import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in original ratings matrix: 100836\n",
      "Number of rows in ratings matrix after removing movies with few ratings: 81116\n",
      "Number of rows in ratings matrix after removing users with few reviews: 81109\n",
      "Number of users: 609\n",
      "Number of movies: 2269\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0       1        1     4.0\n",
       "1       1        3     4.0\n",
       "2       1        6     4.0\n",
       "3       1       47     5.0\n",
       "4       1       50     5.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and preprocess the MovieLens data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path = 'ml-latest-small' # 100K ratings\n",
    "#path = 'ml-1m' # 1 million\n",
    "df_ratings = pd.read_csv(path + '/ratings.csv')\n",
    "print(\"Number of rows in original ratings matrix:\", df_ratings.shape[0])\n",
    "\n",
    "# Remove movies and users with few reviews\n",
    "min_ratings = 10 # (feel free to change this value)\n",
    "df_ratings = df_ratings.groupby('movieId').filter(lambda x : len(x) >= min_ratings)\n",
    "print(\"Number of rows in ratings matrix after removing movies with few ratings:\", df_ratings.shape[0])\n",
    "df_ratings = df_ratings.groupby('userId').filter(lambda x : len(x) >= min_ratings)\n",
    "print(\"Number of rows in ratings matrix after removing users with few reviews:\", df_ratings.shape[0])\n",
    "print(\"Number of users:\", len(df_ratings['userId'].unique()))\n",
    "print(\"Number of movies:\", len(df_ratings['movieId'].unique()))\n",
    "\n",
    "# THINK: Koren et al. used the timestamp to good effect\n",
    "df_ratings.drop('timestamp', axis=1, inplace=True)\n",
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies read from file: 9742\n",
      "After filtering: 2269\n",
      "Number of users: 609\n",
      "Number of movies: 2269\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>406</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>407</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>Heat (1995)</td>\n",
       "      <td>Action|Crime|Thriller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1      406                      Jumanji (1995)   \n",
       "2        2             Grumpier Old Men (1995)   \n",
       "4      407  Father of the Bride Part II (1995)   \n",
       "5        3                         Heat (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "4                                       Comedy  \n",
       "5                        Action|Crime|Thriller  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_ids(values):\n",
    "    # Convert Ids to consecutive integers\n",
    "    newIds = {}\n",
    "    count = 1\n",
    "    for i in values:\n",
    "        if i not in newIds:\n",
    "            newIds[i] = count\n",
    "            count += 1\n",
    "    inv_Ids = dict(map(reversed, newIds.items()))\n",
    "    return newIds, inv_Ids\n",
    "\n",
    "# Read movie descriptions from file\n",
    "df_movies = pd.read_csv(path + '/movies.csv')\n",
    "\n",
    "print(\"Number of movies read from file:\", df_movies.shape[0])\n",
    "df_movies = df_movies[df_movies['movieId'].isin(df_ratings['movieId'].values)]\n",
    "print(\"After filtering:\", df_movies.shape[0])\n",
    "\n",
    "# Make sure that userIds and movieIds are consecutive integers\n",
    "userIds, inv_userIds = convert_ids(df_ratings['userId'].values)\n",
    "df_ratings['userId'] = df_ratings['userId'].apply(lambda x: userIds[x])\n",
    "\n",
    "movieIds, inv_movieIds = convert_ids(df_ratings['movieId'].values)\n",
    "df_ratings['movieId'] = df_ratings['movieId'].apply(lambda x: movieIds[x])\n",
    "df_movies['movieId'] = df_movies['movieId'].apply(lambda x: movieIds[x])\n",
    "\n",
    "print(\"Number of users:\", len(df_ratings['userId'].unique()))\n",
    "print(\"Number of movies:\", len(df_ratings['movieId'].unique()))\n",
    "\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  64887\n",
      "Test set size:  16222\n"
     ]
    }
   ],
   "source": [
    "# Create random train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df_ratings, test_size=0.2, random_state=42) # 10% might be sufficient\n",
    "ratings_train = df_train[['userId','movieId','rating']].values.astype(np.int32)\n",
    "ratings_test = df_test[['userId','movieId','rating']].values.astype(np.int32)\n",
    "print(\"Training set size: \", ratings_train.shape[0])\n",
    "print(\"Test set size: \", ratings_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
