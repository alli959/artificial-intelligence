{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REI602M Machine Learning - Homework 1\n",
    "### Due: Sunday 24.1.2021\n",
    "\n",
    "**Objectives**: Python, NumPy and Matplotlib warmup, gradient descent, linear regression\n",
    "\n",
    "**Name**: (Alexander GuÃ°mundsson), **email:** (alg35@hi.is), **collaborators:** ()\n",
    "\n",
    "Please provide your solutions by filling in the appropriate cells in this notebook, creating new cells as needed. Hand in your solution in PDF format on Gradescope. Make sure that you are familiar with the course rules on collaboration (encouraged) and copying (very, very, bad!)\n",
    "\n",
    "This assignment is somewhat time consuming so start early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) [Python warmup, 15 points] The following code implements the matrix-vector product $y=Ax$ where $A$ is an $n \\times m$ matrix, $x$ is a column vector with $m$ elements and $y$ a column vector with $n$ elements, $y_i = \\sum_{k=1}^m A_{ik}x_k$. (Note that in practice one would use NumPy's `dot` function to perform the matrix-vector multiplication).\n",
    "\n",
    "*Note*: A useful Python/Nympy tutorial which covers most everything we need in REI602M can be found here: https://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 87. 179.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def matvecmul(A, x):\n",
    "    # Computes the matrix-vector product Ax using elementwise operations\n",
    "    n, m = A.shape\n",
    "    assert(m == x.shape[0])\n",
    "    y=np.zeros(n)\n",
    "    for i in range(0, n):\n",
    "        for j in range(0, m):\n",
    "            y[i] = y[i] + A[i,j] * x[j]\n",
    "    return y\n",
    "\n",
    "# Test\n",
    "A=np.array([[1, 2], [3, 4]])\n",
    "x=np.array([5, 41])\n",
    "print(matvecmul(A,x)) # Outputs [87, 179]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Write a Python function which computes the sum of each row in the matrix $A$, i.e. $y_i = \\sum_{j=1}^m A_{ij},~i=1,\\ldots,n$, by accessing individual matrix/vector elements directly as is done in the `matvecmul` function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 2]\n"
     ]
    }
   ],
   "source": [
    "def rowsum(A):\n",
    "    rs = []\n",
    "    for i in A:\n",
    "        s = sum(i)\n",
    "        rs.append(s)\n",
    "    \n",
    "    return rs\n",
    "\n",
    "# Test\n",
    "A=np.array([[1, 2, 3], [3, 4, -5]])\n",
    "print(rowsum(A)) # Outputs [6, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Modify the `rowsum` function so that only positive elements are included in the sum, again by accessing individual matrix elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 2]\n"
     ]
    }
   ],
   "source": [
    "def rowsumpos(A):\n",
    "    rsp = []\n",
    "    for i in A:\n",
    "        s = sum(i)\n",
    "        if s > 0:\n",
    "            rsp.append(s)\n",
    "    return rsp\n",
    "\n",
    "# Test\n",
    "A=np.array([ [1, 2, 3], [3, 4, -5] ])\n",
    "print(rowsumpos(A)) # Outputs [6, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Compute the matrix product $C=AB$ where $A$ is $n \\times m$, $B$ is $m \\times p$ and $C_{ij} = \\sum_{k=1}^m A_{ik} B_{kj}$ is $n \\times p$, by calling `matvecmul` repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2551df66a22d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Outputs [[50, 68], [122,167]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-2551df66a22d>\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(A, B)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatvecmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#Using Xline x YColumn assuming X == A || B and Y == A || B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-575c6df468a6>\u001b[0m in \u001b[0;36mmatvecmul\u001b[0;34m(A, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "def matmul(A,B):\n",
    "    C = []\n",
    "    k = [i[0] for i in B]\n",
    "    print(matvecmul(A, B))\n",
    "    #Using Xline x YColumn assuming X == A || B and Y == A || B\n",
    "    if len(A) > len(B):\n",
    "        print(\"yes\")\n",
    "    \n",
    "\n",
    "    return C\n",
    "\n",
    "# Test\n",
    "A=np.array([[1, 2, 3], [4, 5, 6] ])\n",
    "B=np.array([[7, 10], [8, 11], [9, 12]])\n",
    "print(matmul(A, B)) # Outputs [[50, 68], [122,167]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) [NumPy warmup, 15 points] Repeat a), b) and c) in 1) using NumPy functionality. Aim for fast code by avoiding for-loops as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowsum(A):\n",
    "    # Insert code here\n",
    "    # ...\n",
    "    return rs\n",
    "    \n",
    "def rowsumpos(A):\n",
    "    # Insert code here\n",
    "    # ...\n",
    "    return rsp\n",
    "    \n",
    "def matmul(A,B):\n",
    "    # Insert code here\n",
    "    # ...\n",
    "    return C\n",
    "\n",
    "# Test rowsum, rowsumpos and matmul in the same way as before\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) [Linear regression with gradient descent, 40 points] Here you implement a gradient descent algorithm for linear regression and apply it to a small data set. You then add code to track the minimization process. For many learning algorithms this is a very important part of the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) [30 points] Create a function `linreg_gd` which implements gradient descent for linear regression with the least squares cost function, using maximum number of iterations as a stop criteria.  See the lecture notes for details and the Jupyter notebook `vika01_demo` on Canvas. Try to avoid Python for-loops as much as possible by using NumPy functionality in your final implementation.\n",
    "\n",
    "Use your function to fit a linear regression model on the form\n",
    "$$\n",
    "f_\\theta(x)=\\theta_0 + \\sum_{j=1}^p \\theta_j x_j\n",
    "$$\n",
    "to the `Avertising_centered` dataset provided with this notebook.\n",
    "\n",
    "*Note 1*: You can use the `linear_reg` dataset used in the `vika01_demo` notebook to debug your code. Write the $\\theta$ values to the screen every iteration (or every 100 or 1000 or ...) to monitor convergence. You can compare the output of your code with the values you get by solving the normal equations directly.\n",
    "\n",
    "*Note 2*: You may want to begin by implementing a version that does not rely heavily on NumPy. Once you get it working, gradually introduce NumPy operations into the code.\n",
    "\n",
    "*Note 3*: The advertising dataset is discussed in the ISLR textbook (see sections 2.1, 3.1 and 3.2). Here the data has been transformed by subtracting the mean of each input variable and dividing by its standard deviation so that all inputs now have mean zero and standard deviation one (more on this later in the course). As a result, gradient descent converges faster to the optimal $\\theta$ values but note that the values now differ from those reported in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(\\theta_0, \\theta_1, \\theta_2, \\theta_3) = $ ( ... insert your results here ... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_gd(X, y):\n",
    "    # Insert your code here\n",
    "    # ...\n",
    "    return theta\n",
    "\n",
    "# Load the data\n",
    "# ...\n",
    "\n",
    "# Insert code here\n",
    "# ...\n",
    "\n",
    "# Call your function and report theta values\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) [10 points] Create a plot that shows the value of the cost function, $J(\\theta)$ in each iteration when you apply your gradient descent function to the data in `Advertising_centered`.\n",
    "\n",
    "*Note*: Create a vector J with k_max elements where k_max is the maximum number of iterations. Use `matplotlib.pyplot.plot` or `matplotlib.pyplot.semilogy` to create the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code to generate figure here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) [An alternative cost function for linear regression, 30 points]\n",
    "\n",
    "The least-squares cost function\n",
    "$$\n",
    "    J(\\theta) = \\frac{1}{2}\\sum_{i=1}^n (f_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "is the workhorse of linear regression but it has a significant drawback, namely it is sensitive to 'outliers', data points that differ significantly from the rest. If the prediction, $f_\\theta(x^{(i)})$ differes considerably from the true value $y^{(i)}$, the squared difference will have a large contribution to $J(\\theta)$, magnifying the effect of outlier points. Using the absolute error $|f_\\theta(x^{(i)}) - y^{(i)})|$ instead of the squared error, reduces the effects of outliers but the price to pay is the optimization becomes more difficult.\n",
    "\n",
    "The *log-cosh* cost function\n",
    "$$\n",
    "    J(\\theta) = \\sum_{i=1}^n \\log \\cosh (f_\\theta(x^{(i)}) - y^{(i)})\n",
    "$$\n",
    "alleviates the outlier problem to some extent by behaving like the squred error when the difference between model predictions and data is small but like the absolute error when the difference is large. The log-cosh function is differentiable and can be used in gradient descent algorithms.\n",
    "\n",
    "*Note 1*: Outliers in data can arise for many reasons, they can e.g. represent faulty measurements or simply be due to high variability in the data. Detecting outliers prior to fitting a machine learning model is in general not trivial. A machine learning algorithm should preferrably be robust to the presence of (few) outliers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) [10 points] Derive the gradient for the *log-cosh* cost function\n",
    "\n",
    "*Note 1*: Start with the case $n=1$. The case $n \\geq 1$ follows by noting that the derivative of a sum of functions is equal to the sum of their derivatives.\n",
    "\n",
    "*Note 2*: You can use the LaTeX support in the Jupyter/Colab notebooks (see e.g. this notebook for examples) or simply write down your solution on paper, take a photo with your phone and include as an image below using\n",
    "\n",
    "`<img src=\"mynd.png\" alt=\"drawing\" width=\"300\"/>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert derivation here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) [20 points] Implement a gradient descent algorithm for linear regression that uses the *log-cosh* cost function by modifying your code from problem 3). Test your code on the `outlier.csv` data set using the model\n",
    "$$\n",
    "f_\\theta(x)=\\theta_0 + \\theta_1 x_1\n",
    "$$\n",
    "and compare the results by applying least squares regression to the same data. Create a scatter plot of the data that includes the two regression lines in the plot (see `v01_demo`). What can you conclude from this (single) experiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(\\theta_0, \\theta_1) = $ ( ... insert your results here ... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_gd_logcosh(X, y):\n",
    "    # Insert your code here\n",
    "    # ...\n",
    "    return theta\n",
    "\n",
    "# Load data\n",
    "# ...\n",
    "\n",
    "# Call your function\n",
    "# ...\n",
    "\n",
    "# Draw a figure to compare the results with the least squares solution\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
