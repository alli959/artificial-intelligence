{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REI602M Machine Learning - Homework 4\n",
    "### Due: *Sunday* 14.2.2021\n",
    "\n",
    "**Objectives**: Parameter tuning, Ensemble tree classifiers, Feature importance, Stacking, Pre-processing, performance metrics, scikit-learn and pandas.\n",
    "\n",
    "**Name**: Alexander Gu√∞mundsson, **email: ** alg35@hi.is, **collaborators:** (if any)\n",
    "\n",
    "Please provide your solutions by filling in the appropriate cells in this notebook, creating new cells as needed. Hand in your solution on Gradescope, taking care to locate the appropriate page numbers in the PDF document. Make sure that you are familiar with the course rules on collaboration (encouraged) and copying (very, very, bad)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\\. [Pre-processing and parameter tuning in an SVM classifier, 20 points]\n",
    "The Statlog data set is an old benchmark in machine learning. It contains data from satellite images and the aim is to predict land type (red soil, cotton crop etc). There are 36 integer valued features and 6 classes. The file `sat.trn` contains the 4435 training examples and `sat.tst` contains 2000 test examples. Your task is to obtain an RBF-SVM classifier which obtains high classification accuracy on this data set.\n",
    "\n",
    "a) Evaluate the accuracy of a RBF-SVM obtained with default values for $C$ and $\\gamma$.\n",
    "\n",
    "b) Scale the training set prior to training an RBF-SVM (scale the test data accordingly) and repeat the task from a).\n",
    "\n",
    "c) Use cross-validation on the (scaled) training set to identify optimal values of $C$ and $\\gamma$. You should start with a coarse grid and then do another run with a finer grid. Logarithmically spaced grid values are often used. Evaluate  the accuracy of the resulting classifier by re-training using the best parameter values and report the error on the test set (why is the cross-validation error not a good estimate of the true classifier error here?)\n",
    "\n",
    "d) Using randomized search for hyper-parameter values has been shown to be more efficient than traditional grid search. Instead of evaluating parameter values at regular intervals, the values are sampled from a distribution over possible parameter values. This enables considerable time savings (exhaustive grid search is expensive), or the identification of better parameter values for a given computational budget. Repeat the parameter search using `RandomizedSearchCV`. You can either fix the budget (n_iter parameter) so that the cost is comparable to the exhaustive grid search in c), or set the budget to e.g. 10% of the cost in iii). Report the results of the best classifier.\n",
    "\n",
    "Summarize briefly your findings from a) to d).\n",
    "\n",
    "*Comments*: \n",
    "\n",
    "1) Description of the data set: https://archive.ics.uci.edu/ml/datasets/Statlog+(Landsat+Satellite)\n",
    "\n",
    "2) For scaling the data, see: `preprocessing.StandardScaler` in scikit.\n",
    "\n",
    "3) The parameters $C$ and $\\gamma$ in SVMs are called *hyper-parameters* since they are considered to be fixed during optimization of the model parameters ($\\theta$-values). The procedure of selecting hyper-parameter values is referred to as *model selection* and is typically performed by training the model for several different values of hyper-parameters, and evaluating the resulting model on a (cross-)validation set and finally selecting the values that give the best results.\n",
    "\n",
    "4) An exhaustive search of parameter combinations on a grid is called is referred to as a *grid-search*. The `GridSearchCV` class in scikit-learn makes this easy to do. For details see the scikit documentation, sections 3.1 (Cross-validation: evaluating estimator performance), 3.2 (Tuning the hyper-parameters of an estimator) and the \"Parameter estimation using grid search with cross-validation\" example.\n",
    "\n",
    "5) The paper *Random Search for Hyper-Parameter Optimization* by Bergstra and Bengio describes why randomized search of hyperparamter values is an efficient strategy. https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set accuracy with default values of C and ùõæ =  0.8962795941375423\n",
      "test set accuracy with default values of C and ùõæ =  0.88\n",
      "scaled train set accuracy with default values of C and ùõæ =  0.9095828635851184\n",
      "scaled test set accuracy with default values of C and ùõæ =  0.8895\n",
      "the best parameters for C and ùõæ =  {'C': 1, 'gamma': 0.1}\n",
      "scaled train set accuracy with best values of C and ùõæ =  0.9341600901916572\n",
      "scaled test set accuracy with best values of C and ùõæ =  0.91\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler as std_scl\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "#init train\n",
    "trainData = np.genfromtxt(fname = \"sat.trn\")\n",
    "X_train = trainData[:,:-1]\n",
    "y_train = trainData[:,-1]\n",
    "\n",
    "n,p = X_train.shape\n",
    "\n",
    "X_train = np.c_[np.ones(n),X_train]\n",
    "\n",
    "#init test\n",
    "testData = np.genfromtxt(fname = \"sat.tst\")\n",
    "X_test = testData[:,:-1]\n",
    "y_test = testData[:,-1]\n",
    "\n",
    "n_test,p_test = X_test.shape\n",
    "\n",
    "X_test = np.c_[np.ones(n_test),X_test]\n",
    "\n",
    "\n",
    "#Evaluate the accuracy of a RBF-SVM obtained with default values\n",
    "\n",
    "ev = SVC().fit(X_train,y_train)\n",
    "print(\"train set accuracy with default values of C and ùõæ = \", ev.score(X_train, y_train))\n",
    "print(\"test set accuracy with default values of C and ùõæ = \",ev.score(X_test, y_test))\n",
    "\n",
    "\n",
    "#scale the data\n",
    "X_train_scale = std_scl().fit(X_train).transform(X_train)\n",
    "X_test_scale = std_scl().fit(X_test).transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#Evaluate the accuracy of a RBF-SVM with scaled data\n",
    "ev = SVC().fit(X_train_scale,y_train)\n",
    "print(\"scaled train set accuracy with default values of C and ùõæ = \", ev.score(X_train_scale, y_train))\n",
    "print(\"scaled test set accuracy with default values of C and ùõæ = \", ev.score(X_test_scale, y_test))\n",
    "\n",
    "\n",
    "#cross validating to find C and ùõæ\n",
    "grid = {\n",
    "    'C': [1, 10, 100, 1000],\n",
    "    'gamma': [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,\n",
    "              0.7, 0.8, 0.9, 0.95, 0.99]\n",
    "}\n",
    "\n",
    "grid_search_cv = GridSearchCV(estimator = ev, param_grid = grid, cv = 3)\n",
    "grid_search_cv = grid_search_cv.fit(X_train_scale, y_train)\n",
    "\n",
    "best_params = grid_search_cv.best_params_\n",
    "print(\"the best parameters for C and ùõæ = \",best_params)\n",
    "\n",
    "#Classifying with the best C and ùõæ\n",
    "ev = SVC(C = best_params['C'], gamma = best_params['gamma']).fit(X_train_scale, y_train)\n",
    "print(\"scaled train set accuracy with best values of C and ùõæ = \", ev.score(X_train_scale, y_train))\n",
    "print(\"scaled test set accuracy with best values of C and ùõæ = \", ev.score(X_test_scale, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=SVC(C=1, gamma=0.1),\n",
       "                   param_distributions={'C': [1, 10, 100, 1000],\n",
       "                                        'gamma': [0.01, 0.05, 0.1, 0.2, 0.3,\n",
       "                                                  0.4, 0.5, 0.6, 0.7, 0.8, 0.9,\n",
       "                                                  0.95, 0.99]})"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "RS = RandomizedSearchCV(ev,grid)\n",
    "RS.fit(X_train_scale, y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the results in a-c, we can see that the accuracy increses, cross validation error is not the same as true classifier error, we are cross validating for the train set not the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\\. [Random Forests and feature selection, 20 points]\n",
    "Quantitative Structure - Activity Relationship (QSAR) models relate the activity of chemical compounds to their structural properties. The activity can represent e.g. the potency of a drug or its toxicity. The structural properties may contain basic information such as i) the fraction of carbon atoms in the compound, ii) the spatial arrangement of atoms in the compound and iii) quantities computed from quantum mechanical simulations (*ab-initio* calculations).\n",
    "\n",
    "The QSAR biodegradation Data Set `biodeg.csv` contains 41 molecular descriptors for two groups of compounds, those that are readily biodegradable (RB) and those that are not (NRB). The data set has 356 examples in the RB class and 699 in the NRB class, i.e. it is somewhat unbalanced.\n",
    "\n",
    "a) Using `sklearn.ensemble.RandomForestClassifier`, obtain a classifier for predicting whether a given compound is readily biodegradable or not. Use a random 80/20 train/test set split for evaluting the performance of your classifier. Report the confusion matrix for the test set and calculate the accuracy of the classifier together with *sensitivity* and *specificity* (see below).\n",
    "\n",
    "b) List the names of the 10 *most useful* features for the classification task (see below). Retrain a Random forests classifier using only the 10 most useful features and report sensitivity, specificity and accuracy. How does the performance compare to the classifier trained on the full feature set in a)?\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) The file `biodeg_desc.txt` containes the feature names. A description of the data set can be found here: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation\n",
    "\n",
    "2) Use `sklearn.model_selection.train_test_split` to create a train/test split.\n",
    "\n",
    "3) A correctly predicted RB example is said to be a *true positive* and a correctly predicted NRB examples is said to be a *true negative*. When an NRB example is incorrectly predicted as RB it is a *false positive*. False negatives are defined analogously.\n",
    "\n",
    "The *sensitivity* of a binary classifier is defined as TP/(TP+FN) and the *specificity* is defined as TN/(TN+FP) where TP is the number of true positives etc. These values are conveniently obtained from a confusion matrix, e.g. via `sklearn.metrics.confusion_matrix`\n",
    "\n",
    "4) The feature importance measure provided by the Random Forests implementation in scikit has significant drawbacks. Therefore you will be using a so-called permutation importance measure (see problem 1 for a description). This is implemented in `sklearn.inspection.permutation_importance`.\n",
    "\n",
    "5) Sidenote: Repeatedly retraining a classifier with smaller and smaller number of top features until the validation error (or out-of-bag error) starts to increase can easily lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "                   actual value 1  actual value 0\n",
      "predicted value 1              61              10\n",
      "predicted value 0              14             126 \n",
      "\n",
      "\n",
      "confusion matrix ration\n",
      "                   actual value 1  actual value 0\n",
      "predicted value 1        0.289100        0.047393\n",
      "predicted value 0        0.066351        0.597156 \n",
      "\n",
      "\n",
      "['sensitivity=0.8133333333333334', 'specificity=0.9264705882352942', 'accuracy=0.8862559241706162', 'precision=0.8591549295774648']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#CreateMatrix(y_actual, y_predicted)\n",
    "#loops through the y values and compares them\n",
    "#returns two (nxn) confusion matrixes with count and ratio\n",
    "def createMatrix(y, y_pred):\n",
    "    total = len(y)\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1 and y_pred[i] == 1:\n",
    "            TP += 1\n",
    "        if y[i] == 0 and y_pred[i] == 0:\n",
    "            TN += 1\n",
    "        if y[i] == 0 and y_pred[i] == 1:\n",
    "            FP += 1\n",
    "        if y[i] == 1 and y_pred[i] == 0:\n",
    "            FN += 1\n",
    "    \n",
    "    matrix = np.array([[TP,FP],\n",
    "                      [FN,TN]])\n",
    "    \n",
    "    \n",
    "    matrixR = np.array([[float(TP/total),float(FP/total)],\n",
    "                       [float(FN/total),TN/total]])\n",
    "    \n",
    "    return matrix,matrixR\n",
    "\n",
    "#matrixCalc(matrix)\n",
    "#calculates sensitivity, specifity, accuracy and precision of an matrix\n",
    "#returns dict with keys as \"sensitivity\", \"specificity\", \"accuracy\" and \"precision\"\n",
    "#with the calculations as values\n",
    "def matrixCalc(matrix):\n",
    "    TP = matrix[0,0]\n",
    "    FP = matrix[0,1]\n",
    "    FN = matrix[1,0]\n",
    "    TN = matrix[1,1]\n",
    "    \n",
    "    calc = {}\n",
    "    calc[\"sensitivity\"] = float(TP/(TP+FN))\n",
    "    calc[\"specificity\"] = float(TN/(TN + FP))\n",
    "    calc[\"accuracy\"] = float((TP+TN)/(TP + TN + FP + FN))\n",
    "    calc[\"precision\"] = float(TP/(TP + FP))\n",
    "    \n",
    "    return calc\n",
    "\n",
    "#Insert data\n",
    "Data = np.genfromtxt(fname = \"biodeg.csv\", delimiter = ';')\n",
    "y = Data[:,-1]\n",
    "X = Data[:,0:-1]\n",
    "n,r = X.shape\n",
    "X = np.c_[np.ones(n),X]\n",
    "\n",
    "#classifying\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "random_forest_classifier = RandomForestClassifier().fit(X_train, y_train)\n",
    "y_pred = random_forest_classifier.predict(X_test)\n",
    "\n",
    "#finding accuracy\n",
    "\n",
    "confMatrix, confMatrixR = createMatrix(y_test, y_pred)\n",
    "\n",
    "#drawing the matrix\n",
    "data = {'actual value 1':confMatrix[:,0],\n",
    "       'actual value 0':confMatrix[:,1]}\n",
    "\n",
    "dataR = {'actual value 1':confMatrixR[:,0],\n",
    "       'actual value 0':confMatrixR[:,1]}\n",
    "\n",
    "table = pd.DataFrame(data, index=['predicted value 1', 'predicted value 0'])\n",
    "print(\"confusion matrix\")\n",
    "print(table,'\\n\\n')\n",
    "tableR = pd.DataFrame(dataR, index=['predicted value 1', 'predicted value 0'])\n",
    "print(\"confusion matrix ration\")\n",
    "print(tableR,'\\n\\n')\n",
    "\n",
    "\n",
    "#defining value for calculations\n",
    "calc = matrixCalc(confMatrix)\n",
    "prevAcc = ([x + \"=\" + str(calc[x]) for x in calc])\n",
    "print(prevAcc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top ten most useful features are:  [' Psi_i_A', ' J_Dz(e)', ' nHDon', ' HyWi_B(m)', ' Psi_i_1d', ' F01[N-N]', ' F03[C-O]', ' F03[C-N]', ' nHM', ' nN-N']\n",
      "confusion matrix\n",
      "                   actual value 1  actual value 0\n",
      "predicted value 1              56               6\n",
      "predicted value 0              25             124 \n",
      "\n",
      "\n",
      "confusion matrix ration\n",
      "                   actual value 1  actual value 0\n",
      "predicted value 1        0.265403        0.028436\n",
      "predicted value 0        0.118483        0.587678 \n",
      "\n",
      "\n",
      "['sensitivity=0.691358024691358', 'specificity=0.9538461538461539', 'accuracy=0.8530805687203792', 'precision=0.9032258064516129']\n",
      "\n",
      "\n",
      "\n",
      "previous = ['sensitivity=0.8133333333333334', 'specificity=0.9264705882352942', 'accuracy=0.8862559241706162', 'precision=0.8591549295774648']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "headers = []\n",
    "\n",
    "#getting the names of each label\n",
    "text = open(\"biodeg_desc.txt\", \"r\")\n",
    "for i in range(len(X[0])):\n",
    "    line = text.readline()\n",
    "    if line == \"\":\n",
    "        break\n",
    "    split1 = line.split(')',1)\n",
    "    split2 = split1[1].split(':')\n",
    "    headers.append(split2[0])\n",
    "    \n",
    "top = permutation_importance(random_forest_classifier,X, y)\n",
    "\n",
    "importances_mean = top[\"importances_std\"]\n",
    "importance_sorted = np.argsort(importances_mean)[::-1][:10] #sort and find top 10 indexes\n",
    "\n",
    "print(\"top ten most useful features are: \", [headers[x] for x in importance_sorted])\n",
    "\n",
    "X_new = []\n",
    "\n",
    "#creating new X with only the top features\n",
    "for i in importance_sorted:\n",
    "    X_new.append(X[:,i])\n",
    "\n",
    "X_new = np.array(X_new)\n",
    "X_new = np.transpose(X_new)\n",
    "\n",
    "#classifying\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=100)\n",
    "random_forest_classifier = RandomForestClassifier().fit(X_train, y_train)\n",
    "y_pred = random_forest_classifier.predict(X_test)\n",
    "\n",
    "#finding accuracy\n",
    "\n",
    "confMatrix, confMatrixR = createMatrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "#drawing the matrix\n",
    "data = {'actual value 1':confMatrix[:,0],\n",
    "       'actual value 0':confMatrix[:,1]}\n",
    "\n",
    "dataR = {'actual value 1':confMatrixR[:,0],\n",
    "       'actual value 0':confMatrixR[:,1]}\n",
    "\n",
    "table = pd.DataFrame(data, index=['predicted value 1', 'predicted value 0'])\n",
    "print(\"confusion matrix\")\n",
    "print(table,'\\n\\n')\n",
    "tableR = pd.DataFrame(dataR, index=['predicted value 1', 'predicted value 0'])\n",
    "print(\"confusion matrix ration\")\n",
    "print(tableR,'\\n\\n')\n",
    "\n",
    "\n",
    "#defining value for calculations\n",
    "calc = matrixCalc(confMatrix)\n",
    "print([x + \"=\" + str(calc[x]) for x in calc])\n",
    "print('\\n\\n')\n",
    "\n",
    "print(\"previous =\", prevAcc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there we can see that the sensitivity and accuracy went down but precision and specificity went up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\\. [Stacked regression models, 30 points]\n",
    "In this problem you will construct a *stacked* two-stage regression model for a subset of the Million Song Database (MSD). The data set contains audio features for approximately 500K songs. Each song is represented by 90 features describing its \"timbre\" that are derived from the sampled recordings. The task is to predict the release year of a song.\n",
    "\n",
    "A two-stage stacking model has several regression models in stage 1, all trained on the same data set. Predictions from stage 1 models form a new (derived) data set which is used as input to a single regression model in stage 2. This model \"blends\" predictions from the stage 1 models to create a final prediction, hopefully more accurate than the individual stage 1 predictions.\n",
    "\n",
    "Your stacked regression model will employ Lasso, ExtraTrees, Random Forests and Gradient boosted trees in stage 1 and a linear regression model in stage 2. Training and testing are performed as follows:\n",
    "\n",
    "*Training*: Train each model on the training set, using default parameters to begin with, but increase the number of trees for Extra Trees and Random Forests. Construct a training data set for the stage 2 model by sending the *validation* set (not the original training set) through each of the stage 1 models, resulting in an `n_val` by 4 matrix $X_2$ of prediction values. Train a linear regression model for stage 2 on $(X_2, y_\\text{val})$.\n",
    "\n",
    "*Testing*: Send the test data though all the models in stage 1 to obtain an `n_test` by 4 matrix. The stage 2 linear regression model is used to predict the data in this matrix to obtain the final predictions.\n",
    "\n",
    "Start by creating a histogram of the number of songs per year in the data set to obtain insight into how realistic this prediction task is.\n",
    "\n",
    "a) [20 points] Report the mean-squared error of the individual stage 1 models on the test set and the corresponding $R^2$ coefficient. Construct the stacked regression model described above, report its mean-squared error and $R^2$ coefficient on the test set.\n",
    "\n",
    "b) [10 points] Answer the following questions:\n",
    "\n",
    "i) Are the individual models doing a good job on the prediction task? (Consider the root-mean squared error).\n",
    "\n",
    "ii) Are the individual classifiers failing in some obvious way (failure modes)?\n",
    "\n",
    "iii) Is the stacking procedure worth the extra effort in your opinion? Why or why not?\n",
    "\n",
    "iv) Why is it not a good idea to use the original training set to construct the $X_2$ data set for the stage 2 regression model?\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) Download the subset of the Million Song Databse from here (210 MB): http://archive.ics.uci.edu/ml/datasets/YearPredictionMSD# (mirror: https://notendur.hi.is/steinng/kennsla/2021/ml/data/YearPredictionMSD.zip)\n",
    "\n",
    "2) Use the train, validation and test partitions of the data defined in `load_msd.py`\n",
    "\n",
    "`import load_msd as lmsd\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = lmsd.get_data(ntrain=10000)`\n",
    "\n",
    "3) For Extra Trees and Random Forests you can set `n_jobs=-1` to use multiple cores/processors for training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_msd as lmsd\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = lmsd.get_data(ntrain=10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-189-a5664467fd34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStackingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlvl1_reg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'final_estimator_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;31m# base estimators will be used in transform, predict, and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;31m# predict_proba. They are exposed publicly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[0;32m    146\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_fit_single_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_estimators\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mest\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'drop'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\u001b[0m in \u001b[0;36m_fit_single_estimator\u001b[1;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         n_stages = self._fit_stages(\n\u001b[0m\u001b[0;32m    499\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;31m# fit next stage of trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[0;32m    556\u001b[0m                 \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m                 random_state, X_idx_sorted, X_csc, X_csr)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0m\u001b[0;32m    212\u001b[0m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1240\u001b[0m         \"\"\"\n\u001b[0;32m   1241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lvl1_reg = [\n",
    "    ('etc', ExtraTreesClassifier(n_jobs=-1)),\n",
    "    ('rfc', RandomForestClassifier(n_jobs=-1)),\n",
    "    ('gbc', GradientBoostingClassifier()),\n",
    "]\n",
    "lvl1_class = [\n",
    "    ('lm', Lasso())\n",
    "]\n",
    "\n",
    "\n",
    "lvl2 = LinearRegression()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model = StackingClassifier(estimators = lvl1_reg)\n",
    "#model.fit(X_train, y_train)\n",
    "#print(model.score(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\\. [Preprocessing, performance metrics, 30 points]\n",
    "In this problem you will construct a predictive model for Telemarketing. The data comes from a telemarketing campaign in Portugal where the goal was to get clients to subscribe to long-term savings deposits. The predictive model is to be used to identify customers that are likely to subscribe, based on personal information, economic indicators, whether the client has been contacted before etc. This should make the campaign more effective and reduce marketing costs.\n",
    "\n",
    "The data is in file `bank-additional-full.csv` with a brief description in `bank-additional-names.txt`. The data contains a mixture of continuous and categorical features, with categorical data in text format. Some preprocessing is therefore needed before you can use it with scikit-learn algorithms.\n",
    "\n",
    "The data is time ordered which means that randomly splitting it into training and test sets amounts to peeking into the future and will provide too optimistic estimates of model performance. This is therefore not a suitable evaluation strategy. In situations like this, one uses historical data to train a model and predicts data from the current period. To simulate this scenario you will use the most recent (last) 4000 samples for testing. You then use the remaining samples for training (or a subset thereof).\n",
    "\n",
    "Train a Random Forests or an Extra Trees classifier and evaluate it on the test set using an appropriate performance metric (see below). The selection of metric should take into account whether the classes are balanced or not, as well as the goal of the prediction task. Do you think your model would be useful in practice? Why or why not?\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) This is an open ended problem. There is no single correct answer.\n",
    "\n",
    "2) The data set is described in some detail here:\n",
    "https://archive.ics.uci.edu/ml/datasets/bank+marketing\n",
    "and a previous attempt at predictive modeling in: https://www.sciencedirect.com/science/article/pii/S016792361400061X\n",
    "\n",
    "3) To convert the data into a matrix format suitable for scikit-learn, it is probably easiest to use the Python Data Analysis Library (pandas) package. You load the data using\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "bank_df=pd.read_csv('bank-additional-full.csv',sep=';')```\n",
    "\n",
    "You can iterate over the features using e.g.\n",
    "\n",
    "```python\n",
    "for col in bank_df.columns:\n",
    "    if bank_df[col].dtype == object:\n",
    "        print(\"Categorical: \",col)\n",
    "    else:\n",
    "        print(\"Numerical: \", col)```\n",
    "\n",
    "The output variable (`y`) is 1 if a customer subscribes and 0 otherwise. \n",
    "\n",
    "Start by using only the numerical data. Then add the categorical data gradually. More data does not always help.\n",
    "\n",
    "The simplest conversion of categories to integers is called label encoding. In pandas:\n",
    "\n",
    "```python\n",
    "bank_df['y']=bank_df['y'].astype('category')\n",
    "bank_df['y']=bank_df['y'].cat.codes\n",
    "y=bank_df['y'].values\n",
    "bank_df=bank_df.drop('y',axis=1) # Remove output variable```\n",
    "\n",
    "or using scikit-learn instead:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(bank_df['y'])[:,0]```\n",
    "\n",
    "Label encoding of a feature assumes that the feature values have a natural ordering (are ordinal). This has some drawbacks as detailed in the lecture notes and one-hot-encoding is generally preferred. This is most conveniently done by using `pd.get_dummies` with `drop_first=True`. For this particular data set, direct application of one-hot-encoding does not necessarily improve performance.\n",
    "\n",
    "4) In addition to `sklearn.metrics.confusion_matrix` which can be used to derive sensitivity, specificity and accuracy, the `sklearn.metrics.classification_report` class provides performance metrics called precision recall and F-score (see Wikipedia for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "                   actual value 1  actual value 0\n",
      "predicted value 1             432             137\n",
      "predicted value 0            1423            2008 \n",
      "\n",
      "\n",
      "confusion matrix ration\n",
      "                   actual value 1  actual value 0\n",
      "predicted value 1         0.10800         0.03425\n",
      "predicted value 0         0.35575         0.50200 \n",
      "\n",
      "\n",
      "['sensitivity=0.23288409703504043', 'specificity=0.9361305361305361', 'accuracy=0.61', 'precision=0.7592267135325131']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "#importing the data\n",
    "bank_df = pd.read_csv('bank-additional-full.csv',sep=';')\n",
    "\n",
    "\n",
    "#changing categorical labels to integers\n",
    "for col in bank_df.columns:\n",
    "    if bank_df[col].dtype == object:\n",
    "        bank_df[col]=bank_df[col].astype('category')\n",
    "        bank_df[col]=bank_df[col].cat.codes\n",
    "        \n",
    "#prepering data\n",
    "y = bank_df['y'].values\n",
    "bank_df = bank_df.drop('y', axis=1)\n",
    "X_train = bank_df[0:-4000]\n",
    "y_train = y[0:-4000]\n",
    "X_test = bank_df[-4000:]\n",
    "y_test = y[-4000:]\n",
    "\n",
    "#classifying the data\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier().fit(X_train, y_train)\n",
    "y_pred = random_forest_classifier.predict(X_test)\n",
    "\n",
    "#finding accuracy\n",
    "\n",
    "confMatrix, confMatrixR = createMatrix(y_test, y_pred)\n",
    "\n",
    "#drawing the matrix\n",
    "data = {'actual value 1':confMatrix[:,0],\n",
    "       'actual value 0':confMatrix[:,1]}\n",
    "\n",
    "dataR = {'actual value 1':confMatrixR[:,0],\n",
    "       'actual value 0':confMatrixR[:,1]}\n",
    "\n",
    "table = pd.DataFrame(data, index=['predicted value 1', 'predicted value 0'])\n",
    "print(\"confusion matrix\")\n",
    "print(table,'\\n\\n')\n",
    "tableR = pd.DataFrame(dataR, index=['predicted value 1', 'predicted value 0'])\n",
    "print(\"confusion matrix ration\")\n",
    "print(tableR,'\\n\\n')\n",
    "\n",
    "\n",
    "#defining value for calculations\n",
    "calc = matrixCalc(confMatrix)\n",
    "prevAcc = ([x + \"=\" + str(calc[x]) for x in calc])\n",
    "print(prevAcc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, accuracy is pretty bad, let's take a look at the data to see why that might be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.94      0.72      2145\n",
      "           1       0.76      0.23      0.36      1855\n",
      "\n",
      "    accuracy                           0.61      4000\n",
      "   macro avg       0.67      0.58      0.54      4000\n",
      "weighted avg       0.67      0.61      0.55      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report as c_rep\n",
    "\n",
    "report = c_rep(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the recall and f1 score are very low for classification = 1, let's take a look at the original data to see why it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total numbers of y instances as 1 = 4640\n",
      "total numbers of y instances as 0 = 36548\n"
     ]
    }
   ],
   "source": [
    "print(\"total numbers of y instances as 1 =\",sum(y==1))\n",
    "print(\"total numbers of y instances as 0 =\",sum(y==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the output data is very unbalanced and when classifying you first need to balance the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
